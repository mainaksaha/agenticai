{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e371bc72",
   "metadata": {},
   "source": [
    "# Chain-of-Thought (CoT) Loan Application Demo\n",
    "\n",
    "This Jupyter notebook demonstrates **Chain-of-Thought (CoT) Prompting** in the context of a **loan application**. Our AI agent will \"think aloud\" step-by-step about an applicant’s financial details and then decide whether to approve or deny the loan.\n",
    "\n",
    "We structure the code into **four sections**:\n",
    "1. **Loan Criteria**\n",
    "2. **LLM Adapter**\n",
    "3. **Chain-of-Thought Agent**\n",
    "4. **Main Orchestration**\n",
    "\n",
    "Unlike ReAct, CoT typically performs the entire multi-step reasoning in **one pass**—the agent forms a chain-of-thought and arrives at the conclusion without additional tool calls mid-prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edc5170",
   "metadata": {},
   "source": [
    "## Section 1: Loan Criteria\n",
    "In this section, we store the **business logic** for loan approvals. Here, we define minimum credit score, income thresholds, and a simple check function. In reality, this logic could be more detailed or query external databases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAN_POLICY = {\n",
    "    \"MIN_CREDIT_SCORE\": 650,       # Minimum acceptable credit score\n",
    "    \"MIN_ANNUAL_INCOME\": 30000,   # Minimum annual income in dollars\n",
    "    \"MAX_DEBT_TO_INCOME\": 0.4     # 40% maximum debt-to-income ratio\n",
    "}\n",
    "\n",
    "def basic_loan_eligibility_check(credit_score, annual_income, monthly_debt, monthly_income):\n",
    "    \"\"\"\n",
    "    Returns a dict with high-level checks:\n",
    "    {\n",
    "      'credit_score_check': bool,\n",
    "      'income_check': bool,\n",
    "      'dti_check': bool\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Avoid division by zero\n",
    "    if monthly_income > 0:\n",
    "        dti = monthly_debt / monthly_income\n",
    "    else:\n",
    "        dti = 1.0  # Force fail if monthly_income is zero or negative\n",
    "\n",
    "    results = {\n",
    "        \"credit_score_check\": credit_score >= LOAN_POLICY[\"MIN_CREDIT_SCORE\"],\n",
    "        \"income_check\": annual_income >= LOAN_POLICY[\"MIN_ANNUAL_INCOME\"],\n",
    "        \"dti_check\": dti <= LOAN_POLICY[\"MAX_DEBT_TO_INCOME\"]\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6c6fb",
   "metadata": {},
   "source": [
    "## Section 2: LLM Adapter\n",
    "Here, we define a **placeholder function** that simulates an LLM call (`call_llm`). In a real-world situation, this would be replaced with calls to an actual LLM provider (OpenAI, Anthropic, etc.). For demonstration, it returns a hardcoded chain-of-thought and decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa664e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt, temperature=0.2):\n",
    "    \"\"\"\n",
    "    Simulated LLM call for demonstration.\n",
    "    Returns a chain-of-thought reasoning plus a final conclusion.\n",
    "    \"\"\"\n",
    "    # In an actual application, you'd call your LLM API here.\n",
    "    # Below, we hardcode a mock response when we detect a certain instruction.\n",
    "\n",
    "    if \"Please reason step-by-step\" in prompt:\n",
    "        response = (\n",
    "            \"Chain of Thought:\\n\"\n",
    "            \"1. The applicant's credit score meets the minimum requirement.\\n\"\n",
    "            \"2. The applicant's annual income is slightly above the minimum.\\n\"\n",
    "            \"3. The debt-to-income ratio is within acceptable range.\\n\"\n",
    "            \"Therefore, the loan is approved.\\n\"\n",
    "            \"Final Answer: Approved\\n\"\n",
    "        )\n",
    "        return response\n",
    "    else:\n",
    "        return \"No chain of thought generated.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41791479",
   "metadata": {},
   "source": [
    "## Section 3: Chain-of-Thought Agent\n",
    "Here, we implement the **Chain-of-Thought** flow. We combine some straightforward domain checks (`basic_loan_eligibility_check`) with an **LLM prompt** that instructs the AI to produce **step-by-step reasoning** and a **final decision**.\n",
    "\n",
    "**Key point**: We perform the entire chain-of-thought reasoning in **one pass** of the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoanCoTAgent:\n",
    "    def __init__(self):\n",
    "        self.conversation_log = []  # optional logging\n",
    "\n",
    "    def process_loan_application(self, credit_score, annual_income, monthly_debt, monthly_income):\n",
    "        \"\"\"\n",
    "        1. Run basic checks.\n",
    "        2. Prompt the LLM for chain-of-thought reasoning.\n",
    "        3. Parse and return a final decision.\n",
    "        \"\"\"\n",
    "        # Step 1: Basic checks (domain logic)\n",
    "        eligibility_checks = basic_loan_eligibility_check(\n",
    "            credit_score=credit_score,\n",
    "            annual_income=annual_income,\n",
    "            monthly_debt=monthly_debt,\n",
    "            monthly_income=monthly_income\n",
    "        )\n",
    "\n",
    "        # Build a context string to help the LLM reason about the data\n",
    "        context_info = (\n",
    "            f\"Applicant's credit score: {credit_score}\\n\"\n",
    "            f\"Applicant's annual income: {annual_income}\\n\"\n",
    "            f\"Applicant's monthly debt: {monthly_debt}\\n\"\n",
    "            f\"Applicant's monthly income: {monthly_income}\\n\"\n",
    "            f\"Checks:\\n\"\n",
    "            f\" - credit_score_check: {eligibility_checks['credit_score_check']}\\n\"\n",
    "            f\" - income_check: {eligibility_checks['income_check']}\\n\"\n",
    "            f\" - dti_check: {eligibility_checks['dti_check']}\\n\"\n",
    "        )\n",
    "\n",
    "        # Step 2: Prompt the LLM to produce chain-of-thought reasoning\n",
    "        prompt = (\n",
    "            \"You are a loan approval AI. You have the following data:\\n\"\n",
    "            f\"{context_info}\\n\"\n",
    "            \"Please reason step-by-step and produce a final approval decision. \"\n",
    "            \"If all checks are true, you can lean towards approval. Otherwise, deny.\\n\"\n",
    "            \"Your response should include your chain of thought and a final answer in the form: \"\n",
    "            \"'Final Answer: Approved' or 'Final Answer: Denied'.\\n\"\n",
    "        )\n",
    "\n",
    "        llm_response = call_llm(prompt)\n",
    "        self.conversation_log.append(llm_response)\n",
    "\n",
    "        # Step 3: Extract final decision from the chain-of-thought\n",
    "        if \"Approved\" in llm_response:\n",
    "            decision = \"Approved\"\n",
    "        else:\n",
    "            decision = \"Denied\"\n",
    "\n",
    "        return {\n",
    "            \"chain_of_thought\": llm_response,\n",
    "            \"decision\": decision\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5fb999",
   "metadata": {},
   "source": [
    "## Section 4: Main Orchestration\n",
    "Finally, we tie everything together. We **simulate** a loan application, provide data to the agent, and then observe the chain-of-thought and decision.\n",
    "\n",
    "Feel free to modify the applicant data (credit score, income, debts, etc.) to see different results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample applicant data\n",
    "    applicant_credit_score = 680\n",
    "    applicant_annual_income = 40000.0\n",
    "    applicant_monthly_debt = 800.0\n",
    "    applicant_monthly_income = 3000.0\n",
    "\n",
    "    # Initialize our Chain-of-Thought agent\n",
    "    agent = LoanCoTAgent()\n",
    "\n",
    "    # Process the loan application\n",
    "    result = agent.process_loan_application(\n",
    "        credit_score=applicant_credit_score,\n",
    "        annual_income=applicant_annual_income,\n",
    "        monthly_debt=applicant_monthly_debt,\n",
    "        monthly_income=applicant_monthly_income\n",
    "    )\n",
    "\n",
    "    # Print both the chain of thought and the final decision\n",
    "    print(\"Chain of Thought + Decision:\\n\")\n",
    "    print(result[\"chain_of_thought\"], \"\\n\")\n",
    "    print(f\"Decision: {result['decision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed4a3e",
   "metadata": {},
   "source": [
    "## How to Run\n",
    "1. **Run all cells** in the notebook.\n",
    "2. The output from the final cell should show you a chain-of-thought explanation and a decision (Approved or Denied).\n",
    "3. You can adjust any parameters (credit score, income, debt) in the **Main Orchestration** section to explore different outcomes.\n",
    "\n",
    "## Key Takeaways\n",
    "1. **Chain-of-Thought Prompting**: We explicitly ask the LLM to \"reason step-by-step\" and reveal how it arrives at the decision.\n",
    "2. **Domain Checks + LLM Reasoning**: We combine Python-based guardrails (`basic_loan_eligibility_check`) with an LLM prompt.\n",
    "3. **Single Pass**: Unlike ReAct, there's no mid-prompt tool usage. The LLM does a single pass of multi-step reasoning.\n",
    "4. **Confidentiality**: If needed, you can hide or remove the chain-of-thought from the final user-facing message while still capturing it internally.\n",
    "5. **Extensibility**: You can add more application data (job history, existing loans, etc.) and expand the logic in the same pattern.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "name": "loan_cot_demo"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
