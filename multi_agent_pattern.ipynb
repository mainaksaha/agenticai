{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef0cb33",
   "metadata": {},
   "source": [
    "# Multi-Agent System with OpenAI and Mocked External Tools\n",
    "\n",
    "This notebook demonstrates a **Multi-Agent** approach:\n",
    "1. **Reflexion Agent** – uses **OpenAI** to generate code for CSV analysis, but *executes code locally* in a mock environment.\n",
    "2. **Tool-Using Agent** – queries a **mock** internal knowledge base, no real external calls.\n",
    "3. **Auto-GPT Agent** – calls **OpenAI** for multi-step reasoning about external research, but the \"web search\" is **mocked**.\n",
    "4. **Master Orchestrator** – coordinates the sub-agents, merging their outputs into a final answer.\n",
    "\n",
    "**Goal**: “Perform advanced analysis of EV data from a CSV, gather external info, consult internal knowledge base about EV manufacturing, then produce a final recommendation.”\n",
    "\n",
    "We rely on the **OpenAI** library for any LLM logic and mock all other external calls. This is **conceptual**—in real usage, you might replace these mocks with actual DB queries, web scraping, or file I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2bda8",
   "metadata": {},
   "source": [
    "## 1. OpenAI Setup\n",
    "We'll define a **`call_openai`** function to handle all LLM calls. Make sure you have:\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "and an environment variable `OPENAI_API_KEY` (or set it explicitly in code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=''\n",
    "\n",
    "\n",
    "\n",
    "# Simple adapter to call OpenAI once.\n",
    "# Adjust 'model' to 'gpt-4' or 'gpt-3.5-turbo' as you prefer.\n",
    "\n",
    "def call_openai(system_msg, user_msg, model=\"gpt-4o\", temperature=0.2, max_tokens=200):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg}\n",
    "    ]\n",
    "\n",
    "    client = OpenAI()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c1a1d",
   "metadata": {},
   "source": [
    "## 2. Reflexion Agent\n",
    "We mock:\n",
    "- **Local CSV data** as a `pandas.DataFrame`.\n",
    "- Code generation uses **OpenAI** for snippet creation.\n",
    "- We then **execute** that code in a safe environment.\n",
    "- If there's an **error**, we reflect with the error message back to OpenAI for a corrected snippet.\n",
    "\n",
    "### Implementation Steps\n",
    "1. The agent calls `call_openai` to get an **initial code snippet**.\n",
    "2. **Executes** the code with Python’s `exec(...)` in a restricted local environment.\n",
    "3. If an error, it **reflects** by passing the error back to OpenAI for a correction.\n",
    "4. Repeats until success or we exceed `max_reflections`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflexionCSVAgent:\n",
    "    def __init__(self, max_reflections=2, model=\"gpt-4o\"):\n",
    "        self.max_reflections = max_reflections\n",
    "        self.model = model\n",
    "        self.conversation_log = []\n",
    "\n",
    "    def analyze_data(self, user_query, df):\n",
    "        \"\"\"\n",
    "        Returns analysis result (dictionary or text).\n",
    "        \"\"\"\n",
    "        # 1) Prompt OpenAI for code snippet\n",
    "        system_msg = \"You are a CSV data analysis assistant using Reflexion.\"\n",
    "        user_msg = (\n",
    "            f\"User Query: {user_query}\\n\"\n",
    "            \"Generate Python code to analyze a pandas DataFrame named 'df'.\\n\"\n",
    "            \"Store final output in a variable 'result'.\"\n",
    "        )\n",
    "        code_snippet = call_openai(system_msg, user_msg, model=self.model)\n",
    "        self.conversation_log.append(f\"Initial code:\\n{code_snippet}\\n\")\n",
    "\n",
    "        success, outcome = self.run_code(code_snippet, df)\n",
    "\n",
    "        reflections = 0\n",
    "        while not success and reflections < self.max_reflections:\n",
    "            reflections += 1\n",
    "            error_msg = outcome\n",
    "            self.conversation_log.append(f\"Error encountered: {error_msg}\\n\")\n",
    "\n",
    "            # 2) Reflection step\n",
    "            reflect_msg = (\n",
    "                \"The code failed with error:\\n\" + error_msg + \"\\n\"\n",
    "                \"Please correct the code so it works properly.\"\n",
    "            )\n",
    "            corrected_code = call_openai(system_msg, reflect_msg, model=self.model)\n",
    "            self.conversation_log.append(f\"Corrected code:\\n{corrected_code}\\n\")\n",
    "\n",
    "            success, outcome = self.run_code(corrected_code, df)\n",
    "\n",
    "        if success:\n",
    "            return outcome if outcome else \"No meaningful result.\"\n",
    "        else:\n",
    "            return \"Reflexion agent failed after multiple attempts.\"    \n",
    "\n",
    "    def run_code(self, code_str, df):\n",
    "        # Minimal environment\n",
    "        local_env = {\"df\": df, \"result\": None}\n",
    "        try:\n",
    "            exec(code_str, {}, local_env)\n",
    "            return True, local_env.get(\"result\", \"No result variable.\")\n",
    "        except Exception as e:\n",
    "            return False, str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c8739",
   "metadata": {},
   "source": [
    "## 3. Tool-Using Agent\n",
    "Mocks an **internal knowledge base** about EV manufacturing, battery recycling, etc. No actual external calls. The agent simply returns the relevant text from a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9410ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOCK_KNOWLEDGE_BASE = {\n",
    "    \"EV manufacturing\": \"Production capacity for new EV models is ramping up. Batteries come from multiple suppliers.\",\n",
    "    \"Battery recycling\": \"Company policy emphasizes eco-friendly recycling programs.\"\n",
    "}\n",
    "\n",
    "class ToolUsingAgent:\n",
    "    def __init__(self):\n",
    "        self.conversation_log = []\n",
    "\n",
    "    def query_internal_kb(self, topic):\n",
    "        \"\"\"\n",
    "        Mocks a knowledge base lookup.\n",
    "        \"\"\"\n",
    "        self.conversation_log.append(f\"ToolUsingAgent: Query topic='{topic}'\")\n",
    "        response = MOCK_KNOWLEDGE_BASE.get(topic, \"No info available.\")\n",
    "        self.conversation_log.append(f\"ToolUsingAgent: answer='{response}'\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7392281",
   "metadata": {},
   "source": [
    "## 4. Auto-GPT Agent\n",
    "We **mock** the web search and scraping, but for the **agent’s planning** logic, we use **OpenAI** to produce a short plan. In reality, you might do multiple loops. For brevity, we do a **single** LLM call that either references “best EV review” or “price site.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1915aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOCK_WEB_DATA = {\n",
    "    \"best_ev_reviews.com\": \"The Tesla Model Y is widely considered the best EV by consumer sentiment.\",\n",
    "    \"ev_price_tracker.com\": \"Current average price for Tesla Model Y is about $53,000.\"\n",
    "}\n",
    "\n",
    "def mock_web_search(topic):\n",
    "    sites = []\n",
    "    tl = topic.lower()\n",
    "    if \"best ev\" in tl or \"top ev\" in tl:\n",
    "        sites.append(\"best_ev_reviews.com\")\n",
    "    if \"price\" in tl:\n",
    "        sites.append(\"ev_price_tracker.com\")\n",
    "    if not sites:\n",
    "        sites.append(\"general_ev_site.com\")\n",
    "    return sites\n",
    "\n",
    "def mock_web_scrape(url):\n",
    "    return MOCK_WEB_DATA.get(url, \"No relevant info.\")\n",
    "\n",
    "class AutoGPTAgent:\n",
    "    def __init__(self, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.conversation_log = []\n",
    "\n",
    "    def research_topic(self, topic):\n",
    "        \"\"\"\n",
    "        Uses OpenAI to decide which sites to \"visit\" (mock), then returns aggregated data.\n",
    "        \"\"\"\n",
    "        system_msg = \"You are an Auto-GPT style agent focusing on EV research.\"        \n",
    "        user_msg = f\"User wants external info on: {topic}. Decide which sites to visit (like best_ev_reviews.com or ev_price_tracker.com).\"\n",
    "\n",
    "        # Let the LLM produce a plan\n",
    "        plan_response = call_openai(system_msg, user_msg, model=self.model, max_tokens=150)\n",
    "        self.conversation_log.append(f\"Plan: {plan_response}\")\n",
    "\n",
    "        # We’ll do a simple keyword check to see if it suggests a best EV or a price check.\n",
    "        sites_to_visit = []\n",
    "        if \"best_ev_reviews.com\" in plan_response.lower():\n",
    "            sites_to_visit.append(\"best_ev_reviews.com\")\n",
    "        if \"ev_price_tracker.com\" in plan_response.lower():\n",
    "            sites_to_visit.append(\"ev_price_tracker.com\")\n",
    "\n",
    "        # fallback\n",
    "        if not sites_to_visit:\n",
    "            sites_to_visit = mock_web_search(topic)\n",
    "\n",
    "        combined_data = []\n",
    "        for site in sites_to_visit:\n",
    "            data = mock_web_scrape(site)\n",
    "            combined_data.append(data)\n",
    "\n",
    "        return \"\\n\".join(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9927efc",
   "metadata": {},
   "source": [
    "## 5. Multi-Agent Orchestrator\n",
    "Ties everything together:\n",
    "1. Calls **ReflexionCSVAgent** to process local CSV.\n",
    "2. Calls **ToolUsingAgent** to get internal info.\n",
    "3. Calls **AutoGPTAgent** to do a (mock) external search.\n",
    "4. Merges results into a final recommendation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentOrchestrator:\n",
    "    def __init__(self, model=\"gpt-4o\"):\n",
    "        self.reflexion_agent = ReflexionCSVAgent(model=model)\n",
    "        self.tool_agent = ToolUsingAgent()\n",
    "        self.autogpt_agent = AutoGPTAgent(model=model)\n",
    "\n",
    "    def handle_request(self, user_query, df):\n",
    "        \"\"\"\n",
    "        Steps:\n",
    "        1) Reflexion agent -> analyze CSV.\n",
    "        2) Tool-Using -> query internal KB.\n",
    "        3) Auto-GPT -> external research.\n",
    "        4) Merge final.\n",
    "        \"\"\"\n",
    "        analysis_result = self.reflexion_agent.analyze_data(user_query, df)\n",
    "        internal_info = self.tool_agent.query_internal_kb(\"EV manufacturing\")\n",
    "        external_data = self.autogpt_agent.research_topic(\"Best EV and price\")\n",
    "\n",
    "        return self._combine(analysis_result, internal_info, external_data)\n",
    "\n",
    "    def _combine(self, analysis_result, internal_info, external_info):\n",
    "        # Summarize\n",
    "        if isinstance(analysis_result, dict):\n",
    "            summary_text = (\n",
    "                f\"Local CSV Analysis:\\n\"\n",
    "                f\" - Max Range: {analysis_result.get('max_range', 'N/A')}\\n\"\n",
    "                f\" - Average Price: {analysis_result.get('avg_price', 'N/A')}\\n\\n\"\n",
    "            )\n",
    "        else:\n",
    "            summary_text = f\"Local CSV Analysis: {analysis_result}\\n\\n\"\n",
    "\n",
    "        summary_text += f\"Internal Knowledge: {internal_info}\\n\\n\"\n",
    "        summary_text += f\"External Research:\\n{external_info}\\n\\n\"\n",
    "\n",
    "        recommendation = (\n",
    "            \"Based on local CSV data, internal manufacturing knowledge, and external market info,\\n\"\n",
    "            \"the best EV to focus on is likely the Tesla Model Y, given highest range and strong user sentiment.\"\n",
    "        )\n",
    "\n",
    "        return summary_text + recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b5e28",
   "metadata": {},
   "source": [
    "## 6. Main\n",
    "We create a sample CSV DataFrame, set a user query, and run the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138eb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Mock CSV data\n",
    "    sample_data = {\n",
    "        \"Model\": [\"EV A\", \"EV B\", \"Tesla Model Y\", \"EV C\"],\n",
    "        \"Range\": [250, 220, 300, 280],\n",
    "        \"Price\": [40000, 35000, 50000, 45000]\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "\n",
    "    # High-level user request\n",
    "    user_query = \"Perform advanced analysis of EV data, get external info, and consult internal knowledge about EV manufacturing.\" \n",
    "\n",
    "    orchestrator = MultiAgentOrchestrator(model=\"gpt-4o\")\n",
    "    final_answer = orchestrator.handle_request(user_query, df)\n",
    "\n",
    "    print(\"=== FINAL ANSWER ===\")\n",
    "    print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f42b2",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "1. **ReflexionCSVAgent** uses OpenAI to generate code, runs it locally. If an error occurs, it passes the error message back to OpenAI for a corrected snippet.\n",
    "2. **ToolUsingAgent** just returns a string from a dictionary—mocking an \"internal KB.\" No external calls.\n",
    "3. **AutoGPTAgent** calls OpenAI for a quick \"plan\" on which sites to visit, but the web searching and scraping are **mocked** (`mock_web_search`, `mock_web_scrape`).\n",
    "4. The **Orchestrator** calls each sub-agent, merges results into a single user-facing message.\n",
    "\n",
    "## Usage\n",
    "1. **Install** `openai` (`pip install openai`).\n",
    "2. **Set** your `OPENAI_API_KEY`.\n",
    "3. **Run** all cells. The final cell prints the combined answer.\n",
    "\n",
    "## Key Takeaways\n",
    "- Only **OpenAI** calls are real (for code gen, reflection, and mini \"auto-gpt\" steps).\n",
    "- All “tools” (CSV exec, knowledge base, web scraping) are mocked.\n",
    "- This design shows how multiple specialized agents can be orchestrated in a single system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "name": "multi_agent_demo_openai"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
